"""
Production-Grade Text Normalization Engine
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Purpose: Eliminate scammer obfuscation techniques before detection
Architecture: 8-stage deterministic pipeline
Performance: ~0.1ms per message
Security: No eval(), no exec(), no external calls

Pipeline Flow:
Raw Input â†’ Unicode â†’ Zero-Width â†’ Control Chars â†’ Homoglyphs â†’ 
Leetspeak â†’ URL Deobfuscation â†’ Whitespace â†’ Lowercase â†’ Output
"""

import re
import unicodedata
from typing import Dict, List, Tuple
from functools import lru_cache

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PRECOMPILED PATTERNS (Performance Optimization)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Zero-width characters (INVISIBLE UNICODE)
ZERO_WIDTH_PATTERN = re.compile(
    r'[\u200B\u200C\u200D\u200E\u200F\uFEFF\u2060\u180E]'
)

# Control characters (ASCII 0-31 except whitespace)
CONTROL_CHARS_PATTERN = re.compile(r'[\x00-\x1F\x7F]')

# Multiple whitespace consolidation
MULTI_WHITESPACE_PATTERN = re.compile(r'\s+')

# URL detection for targeted deobfuscation
URL_DETECTION_PATTERN = re.compile(
    r'(?:hxxp|h[tx]{2}ps?|www)[^\s]*',
    re.IGNORECASE
)

# Digit sequences (for phone number normalization)
PHONE_PATTERN = re.compile(r'[-\s().]')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEETSPEAK & OBFUSCATION MAPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LEET_MAP: Dict[str, str] = {
    # Numbers
    "0": "o",
    "1": "i",
    "3": "e",
    "4": "a",
    "5": "s",
    "7": "t",
    "8": "b",
    "9": "g",
    
    # Special characters
    "@": "a",
    "$": "s",
    "â‚¬": "e",
    "Â£": "l",
    "Â¥": "y",
    "â‚¹": "r",
    
    # Brackets/math
    "|": "i",
    "!": "i",
    "()": "o",
    "[]": "i",
    "{}": "o",
    "<>": "o",
    "/-\\": "a",
    "|\\|": "n",
    "|\\/|": "m",
    "\\|/": "v",
}

# URL obfuscation patterns (scammers use these to evade filters)
URL_OBFUSCATIONS: Dict[str, str] = {
    # Protocol obfuscation
    "hxxp": "http",
    "hxxps": "https",
    "h**p": "http",
    "h**ps": "https",
    "ht_tp": "http",
    "ht_tps": "https",
    
    # Dot obfuscation
    "[.]": ".",
    "(.)": ".",
    "{.}": ".",
    "< . >": ".",
    " dot ": ".",
    " DOT ": ".",
    "_dot_": ".",
    "-dot-": ".",
    "[dot]": ".",
    "(dot)": ".",
    
    # Slash obfuscation
    "\\/": "/",
    "//": "/",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HOMOGLYPH NORMALIZATION (Unicode lookalikes)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HOMOGLYPH_MAP: Dict[str, str] = {
    # Cyrillic â†’ Latin (scammers use Cyrillic to bypass filters)
    "Ğ°": "a",  # Cyrillic A
    "Ğµ": "e",  # Cyrillic E
    "Ğ¾": "o",  # Cyrillic O
    "Ñ€": "p",  # Cyrillic P
    "Ñ": "c",  # Cyrillic C
    "Ñƒ": "y",  # Cyrillic Y
    "Ñ…": "x",  # Cyrillic X
    "Ñ–": "i",  # Ukrainian I
    "Ó": "l",  # Palochka
    "Ñ˜": "j",  # Cyrillic J
    "Ñ•": "s",  # Cyrillic S
    "Ò»": "h",  # Shha
    "Ô": "d",  # Komi De
    
    # Greek â†’ Latin
    "Î±": "a",
    "Î²": "b",
    "Î³": "g",
    "Î´": "d",
    "Îµ": "e",
    "Î¶": "z",
    "Î·": "n",
    "Î¸": "o",
    "Î¹": "i",
    "Îº": "k",
    "Î»": "l",
    "Î¼": "u",
    "Î½": "v",
    "Î¾": "e",
    "Î¿": "o",
    "Ï€": "n",
    "Ï": "p",
    "Ïƒ": "o",
    "Ï„": "t",
    "Ï…": "u",
    "Ï†": "f",
    "Ï‡": "x",
    "Ïˆ": "ps",
    "Ï‰": "w",
    
    # Mathematical â†’ Latin
    "â„Š": "g",
    "â„": "h",
    "â„“": "l",
    "â„˜": "p",
    "â„›": "r",
    "â„¯": "e",
    "â„´": "o",
    
    # Fullwidth â†’ ASCII
    "ï¼¡": "a", "ï¼¢": "b", "ï¼£": "c", "ï¼¤": "d", "ï¼¥": "e",
    "ï¼¦": "f", "ï¼§": "g", "ï¼¨": "h", "ï¼©": "i", "ï¼ª": "j",
    "ï¼«": "k", "ï¼¬": "l", "ï¼­": "m", "ï¼®": "n", "ï¼¯": "o",
    "ï¼°": "p", "ï¼±": "q", "ï¼²": "r", "ï¼³": "s", "ï¼´": "t",
    "ï¼µ": "u", "ï¼¶": "v", "ï¼·": "w", "ï¼¸": "x", "ï¼¹": "y", "ï¼º": "z",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HINDI/DEVANAGARI NORMALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEVANAGARI_VARIATIONS: Dict[str, str] = {
    # Common typing variations in Hindi
    "à¤•à¤¼": "à¤•",
    "à¤–à¤¼": "à¤–",
    "à¤—à¤¼": "à¤—",
    "à¤œà¤¼": "à¤œ",
    "à¤¡à¤¼": "à¤¡",
    "à¤¢à¤¼": "à¤¢",
    "à¤«à¤¼": "à¤«",
    "à¤¯à¤¼": "à¤¯",
    
    # Nukta normalization (combining marks)
    "\u093C": "",  # Nukta combining mark
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 1: Unicode Normalization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@lru_cache(maxsize=1024)
def normalize_unicode(text: str) -> str:
    """
    Apply Unicode NFKC normalization.
    
    NFKC = Normalization Form KC (Compatibility Composition)
    - Converts ligatures to base characters
    - Converts superscripts/subscripts to normal
    - Converts fullwidth chars to halfwidth
    
    Example: â„€ â†’ a/c, ï¬ â†’ fi, Â² â†’ 2
    """
    return unicodedata.normalize("NFKC", text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 2: Zero-Width Character Removal
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def remove_zero_width(text: str) -> str:
    """
    Remove invisible zero-width characters.
    
    Scammers use these to split keywords:
    "pâ€‹aâ€‹yâ€‹pâ€‹aâ€‹l" â†’ "paypal"
    """
    return ZERO_WIDTH_PATTERN.sub("", text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 3: Control Character Removal
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def remove_control_characters(text: str) -> str:
    """
    Remove non-printable ASCII control characters (0x00-0x1F, 0x7F).
    Preserves: \t (tab), \n (newline), \r (return).
    """
    # Keep tabs and newlines, remove others
    result = []
    for char in text:
        code = ord(char)
        if code in (9, 10, 13):  # Tab, LF, CR
            result.append(char)
        elif 32 <= code <= 126 or code >= 128:  # Printable
            result.append(char)
    return "".join(result)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 4: Homoglyph Normalization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_homoglyphs(text: str) -> str:
    """
    Replace visually similar characters with ASCII equivalents.
    
    Example: "Ñ€Ğ°ÑƒÑ€Ğ°l" (Cyrillic) â†’ "paypal" (Latin)
    """
    for fake, real in HOMOGLYPH_MAP.items():
        text = text.replace(fake, real)
    
    # Devanagari variations
    for fake, real in DEVANAGARI_VARIATIONS.items():
        text = text.replace(fake, real)
    
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 5: Leetspeak Conversion
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_leetspeak(text: str) -> str:
    """
    Context-aware leetspeak conversion.
    
    Converts: "Fr33" â†’ "free", "p@yp@l" â†’ "paypal"
    Preserves: "+91-9876543210" (phone numbers), "https://site.com" (URLs)
    
    Strategy:
    1. Protect numeric sequences (phone numbers, IDs, etc.)
    2. Only convert numbers when they're clearly part of words
    3. Always convert symbol substitutions (@, $, etc.)
    """
    
    # Patterns to protect from conversion
    PROTECT_PATTERNS = [
        (r'\+?\d{10,}', '___PHONE___'),           # Phone numbers: +919876543210
        (r'\b\d{8,}\b', '___NUMBER___'),          # Long numeric sequences: 12345678
        (r'https?://[^\s]+', '___URL___'),        # URLs
        (r'\b\d{4}-\d{4}-\d{4,}\b', '___ACCOUNT___'),  # Account patterns
    ]
    
    # Step 1: Replace protected patterns with placeholders
    protected = {}
    counter = 0
    for pattern, placeholder in PROTECT_PATTERNS:
        matches = re.findall(pattern, text)
        for match in matches:
            key = f"{placeholder}{counter}"
            protected[key] = match
            text = text.replace(match, key, 1)
            counter += 1
    
    # Step 2: Apply multi-character leetspeak replacements
    for fake, real in sorted(LEET_MAP.items(), key=lambda x: -len(x[0])):
        if len(fake) > 1:
            text = text.replace(fake, real)
    
    # Step 3: Apply single-character replacements (symbols always, numbers contextually)
    # Always convert symbols
    SYMBOL_LEET = {"@": "a", "$": "s", "â‚¬": "e", "Â£": "l", "Â¥": "y", "â‚¹": "r", "|": "i"}
    for fake, real in SYMBOL_LEET.items():
        text = text.replace(fake, real)
    
    # Convert numbers ONLY when part of words (has letters nearby)
    NUMBER_LEET = {"0": "o", "1": "i", "3": "e", "4": "a", "5": "s", "7": "t", "8": "b", "9": "g"}
    
    # Use word boundary detection - only convert numbers inside words
    result = []
    i = 0
    while i < len(text):
        char = text[i]
        
        # If it's a number that could be leetspeak
        if char in NUMBER_LEET:
            # Check if it's part of a word (has letters before or after)
            has_letter_before = i > 0 and text[i-1].isalpha()
            has_letter_after = i < len(text)-1 and text[i+1].isalpha()
            
            # Only convert if it's clearly part of a word
            if has_letter_before or has_letter_after:
                result.append(NUMBER_LEET[char])
            else:
                result.append(char)
        else:
            result.append(char)
        i += 1
    
    text = ''.join(result)
    
    # Step 4: Restore protected patterns
    for key, original in protected.items():
        text = text.replace(key, original)
    
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 6: URL Deobfuscation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def deobfuscate_urls(text: str) -> str:
    """
    Fix common URL obfuscation patterns.
    
    Example: "hxxps://paypĞ°l[.]com" â†’ "https://paypal.com"
    """
    # Apply all obfuscation replacements
    for fake, real in URL_OBFUSCATIONS.items():
        text = text.replace(fake, real)
    
    # Additional URL-specific cleaning
    text = re.sub(r'h\*{2,}ps?', 'https', text, flags=re.IGNORECASE)
    text = re.sub(r'\s*dot\s*', '.', text, flags=re.IGNORECASE)
    
    # Fix any remaining hxxp/hxxps patterns
    text = text.replace("hxxp://", "http://")
    text = text.replace("hxxps://", "https://")
    text = text.replace("http:/", "http://")
    text = text.replace("https:/", "https://")
    
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 7: Whitespace Normalization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_whitespace(text: str) -> str:
    """
    Collapse multiple whitespace characters into single space.
    Remove leading/trailing whitespace.
    
    Example: "urgent   action  needed" â†’ "urgent action needed"
    """
    # Replace all whitespace variants with single space
    text = MULTI_WHITESPACE_PATTERN.sub(" ", text)
    return text.strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STAGE 8: Phone Number Normalization (Helper)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_phone_number(text: str) -> str:
    """
    Normalize phone number formatting.
    
    Example: "+91 (987) 654-3210" â†’ "+919876543210"
    """
    return PHONE_PATTERN.sub("", text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN NORMALIZATION ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_input(text: str) -> str:
    """
    Production-grade 8-stage normalization pipeline.
    
    âœ… Deterministic (same input â†’ same output)
    âœ… Idempotent (running twice = same result)
    âœ… Fast (~0.1ms per message)
    âœ… Safe (no eval, no exec)
    
    Args:
        text: Raw user input (potentially obfuscated)
    
    Returns:
        Normalized, lowercase, clean text ready for detection
    
    Example:
        Input:  "Frâ€‹33 BÑ–tcĞ¾in!!! ClÑ–ck hxxps://Ñ€Ğ°ÑƒÑ€Ğ°l[.]com"
        Output: "free bitcoin!!! click https://paypal.com"
    """
    
    # Type safety
    if not isinstance(text, str):
        return ""
    
    # Empty check
    if not text.strip():
        return ""
    
    # Execute pipeline
    text = normalize_unicode(text)              # Stage 1
    text = remove_zero_width(text)              # Stage 2
    text = remove_control_characters(text)      # Stage 3
    text = normalize_homoglyphs(text)           # Stage 4
    text = normalize_leetspeak(text)            # Stage 5
    text = deobfuscate_urls(text)               # Stage 6
    text = normalize_whitespace(text)           # Stage 7
    text = text.lower()                         # Stage 8
    
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADVANCED: Selective Normalization (for specific fields)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_for_detection(text: str) -> str:
    """
    Alias for normalize_input (primary use case).
    """
    return normalize_input(text)


def normalize_for_display(text: str) -> str:
    """
    Lighter normalization for UI display (preserve readability).
    """
    text = normalize_unicode(text)
    text = remove_zero_width(text)
    text = normalize_whitespace(text)
    return text


def normalize_url_for_extraction(url: str) -> str:
    """
    Specialized normalization for URL extraction.
    """
    url = deobfuscate_urls(url)
    url = normalize_homoglyphs(url)
    url = url.lower()
    return url


def normalize_phone_for_extraction(phone: str) -> str:
    """
    Specialized normalization for phone number extraction.
    """
    phone = normalize_phone_number(phone)
    phone = re.sub(r'[^\d+]', '', phone)
    return phone


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DIAGNOSTICS & DEBUG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_normalization_report(text: str) -> Dict[str, str]:
    """
    Debug function: Show transformation at each stage.
    
    Returns:
        Dictionary with results from each normalization stage
    """
    report = {
        "original": text,
        "stage1_unicode": normalize_unicode(text),
    }
    
    current = report["stage1_unicode"]
    
    current = remove_zero_width(current)
    report["stage2_zero_width"] = current
    
    current = remove_control_characters(current)
    report["stage3_control_chars"] = current
    
    current = normalize_homoglyphs(current)
    report["stage4_homoglyphs"] = current
    
    current = normalize_leetspeak(current)
    report["stage5_leetspeak"] = current
    
    current = deobfuscate_urls(current)
    report["stage6_urls"] = current
    
    current = normalize_whitespace(current)
    report["stage7_whitespace"] = current
    
    current = current.lower()
    report["stage8_final"] = current
    
    return report


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERFORMANCE TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Test cases
    test_cases = [
        "Frâ€‹33 BÑ–tcĞ¾in!!!",
        "ClÑ–ck hxxps://Ñ€Ğ°ÑƒÑ€Ğ°l[.]com",
        "Urğ“°ğ“®ğ“·ğ“½ @cti0n n3eded",
        "CĞ°ll +91 (987) 654-3210",
        "à¤†à¤ªà¤•à¤¾ à¤–à¤¾à¤¤à¤¾ à¤¬à¤‚à¤¦ à¤¹à¥‹ à¤œà¤¾à¤à¤—à¤¾",
        "Ğ£Ğ I à¤­à¥‡à¤œà¥‹ 1000â‚¹",
    ]
    
    print("â•" * 70)
    print("NORMALIZATION MODULE - TEST RUN")
    print("â•" * 70)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nTest {i}:")
        print(f"  Input:  {test!r}")
        normalized = normalize_input(test)
        print(f"  Output: {normalized!r}")
    
    print("\n" + "â•" * 70)
    print("DETAILED REPORT FOR TEST 1:")
    print("â•" * 70)
    report = get_normalization_report(test_cases[0])
    for stage, result in report.items():
        print(f"{stage:20s}: {result!r}")
